{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHO6AESuxnGM"
      },
      "source": [
        "# Trabalho 1 - Nivelamento\n",
        "\n",
        "Considere os datasets abaixo, estratégias de pré-processamento, medidas de avaliação, métodos de comparação estatística e os seguintes algoritmos de aprendizado de máquina: árvore de decisão, random forest e k-nearest neighbor. A partir disso, responda as seguintes perguntas:\n",
        "\n",
        "1. Qual o algoritmo de AM mais adequado para cada problema?\n",
        "2. Qual o algoritmo de AM mais adequado para todos os problemas?\n",
        "\n",
        "Para responder essas questões construa um notebook no colab ou um ambiente similar. Documente de forma clara cada passo e justifique suas decisões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hutz7IVAxuxv"
      },
      "source": [
        "### Datasets\n",
        "Considere os seguintes datasets:\n",
        "\n",
        "+ **Arrhythmia:** Binarized version of the original data set (see version 1). The multi-class target feature is converted to a two-class nominal target feature by re-labeling the majority class as positive ('P') and all others as negative ('N'). [link](https://www.openml.org/search?type=data&sort=runs&id=1017&status=active)\n",
        "\n",
        "+ **Airlines:** Airlines Dataset Inspired in the regression dataset from Elena Ikonomovska. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure. [link] (https://www.openml.org/search?type=data&sort=runs&id=1169&status=active)\n",
        "\n",
        "+ **Phoneme:** The aim of this dataset is to distinguish between nasal (class 0) and oral sounds (class 1). Five different attributes were chosen to characterize each vowel: they are the amplitudes of the five first harmonics AHi, normalised by the total energy Ene (integrated on all the frequencies): AHi/Ene. The phonemes are transcribed as follows: sh as in she, dcl as in dark, iy as the vowel in she, aa as the vowel in dark, and ao as the first vowel in water. [link] (https://www.openml.org/search?type=data&sort=runs&id=1489&status=active)\n",
        "\n",
        "+ **Phishing Websites:** One of the challenges faced by our research was the unavailability of reliable training datasets. In fact this challenge faces any researcher in the field. However, although plenty of articles about predicting phishing websites have been disseminated these days, no reliable training dataset has been published publically, may be because there is no agreement in literature on the definitive features that characterize phishing webpages, hence it is difficult to shape a dataset that covers all possible features. In this dataset, we shed light on the important features that have proved to be sound and effective in predicting phishing websites. [link](https://www.openml.org/search?type=data&sort=runs&id=4534&status=active)\n",
        "\n",
        "+ **Satellite:** The satellite dataset comprises of features extracted from satellite observations. In particular, each image was taken under four different light wavelength, two in visible light (green and red) and two infrared images. The task of the original dataset is to classify the image into the soil category of the observed region. [link](https://www.openml.org/search?type=data&sort=runs&id=40900&status=active)\n",
        "\n",
        "+ **Adult:** Prediction task is to determine whether a person makes over 50K a year. Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0)).[link](https://www.openml.org/search?type=data&sort=runs&id=1590&status=active)\n",
        "\n",
        "+ **AedesSex:** This dataset has features of sounds gathered using a infrared sensor from _Ae. aegypti_ mosquitoes. The objetive is to classify males and females mosquitoes from their sounds. **Sex** feature is the label. [link](https://github.com/denismr/Classification-and-Counting-with-Recurrent-Contexts/blob/master/codeAndData/data/AedesSex.csv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ExXJnto2O4"
      },
      "source": [
        "### Nome: Jedson Gabriel Ferreira de Paula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arrhythmia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Loading and first look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOdk7BJHxmH4"
      },
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "data = arff.loadarff('./datasets/arrhythmia.arff')\n",
        "df_arrhythmia = pd.DataFrame(data[0])\n",
        "\n",
        "df_arrhythmia.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_arrhythmia.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install skimpy\n",
        "from skimpy import skim\n",
        "\n",
        "skim(df_arrhythmia)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# removing columns with any missing values\n",
        "df_arrhythmia = df_arrhythmia.dropna(axis='columns')\n",
        "\n",
        "df_arrhythmia.isna().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_obj_cols = df_arrhythmia.columns[df_arrhythmia.dtypes == object]\n",
        "\n",
        "df_arrhythmia[arr_obj_cols].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_unary_cols = []\n",
        "\n",
        "for col in arr_obj_cols:\n",
        "    # if is a binary column\n",
        "    if len(df_arrhythmia[col].unique()) == 2:\n",
        "        bin_values = df_arrhythmia[col].unique()\n",
        "        df_arrhythmia[col] = df_arrhythmia[col].map({bin_values[0]: 0, bin_values[1]: 1})\n",
        "    # if column is unary\n",
        "    elif len(df_arrhythmia[col].unique()) == 1:\n",
        "        arr_unary_cols.append(col)\n",
        "    # print problematic column\n",
        "    else:\n",
        "        print(col, len(df_arrhythmia[col].unique()))\n",
        "\n",
        "# removing unary cols\n",
        "df_arrhythmia = df_arrhythmia.drop(columns=arr_unary_cols)\n",
        "\n",
        "arr_obj_cols = [col for col in arr_obj_cols.values if col not in arr_unary_cols]\n",
        "df_arrhythmia[arr_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_arrhythmia['binaryClass'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_bal = df_arrhythmia['binaryClass'].value_counts()[0] / df_arrhythmia['binaryClass'].count()\n",
        "print(f\"{arr_bal * 100:.2f}% {(1 - arr_bal) * 100:.2f}% of disbalancing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_X = df_arrhythmia.drop(columns=['binaryClass'])\n",
        "arr_y = df_arrhythmia['binaryClass']\n",
        "\n",
        "arr_X = (arr_X - arr_X.mean()) / arr_X.std()\n",
        "\n",
        "arr_X = arr_X.dropna(axis='columns')\n",
        "\n",
        "arr_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "arr_dt_params_file = \"./params/arr_decision_tree_grided_cv.json\"\n",
        "arr_dt_scores = []\n",
        "\n",
        "if not exists(arr_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    arr_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    arr_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    arr_decision_tree_grided_cv = GridSearchCV(arr_decision_tree_clf, arr_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    arr_decision_tree_grided_cv.fit(arr_X, arr_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(arr_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(arr_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        arr_dt_scores = cross_val_score(arr_decision_tree_grided_cv, arr_X, arr_y, cv=10)\n",
        "        print(arr_dt_scores)\n",
        "        plot_confusion_matrix(arr_decision_tree_grided_cv, arr_X, arr_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(arr_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        arr_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        arr_decision_tree_grided_cv = arr_decision_tree_clf.fit(arr_X, arr_y)\n",
        "\n",
        "        arr_dt_scores = cross_val_score(arr_decision_tree_grided_cv, arr_X, arr_y, cv=10)\n",
        "        print(arr_dt_scores)\n",
        "        plot_confusion_matrix(arr_decision_tree_grided_cv, arr_X, arr_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "arr_rf_params_file = \"./params/arr_random_forest_grided_cv.json\"\n",
        "arr_rf_scores = []\n",
        "\n",
        "if not exists(arr_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    arr_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    arr_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    arr_random_forest_grided_cv = GridSearchCV(arr_random_forest_clf, arr_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    arr_random_forest_grided_cv.fit(arr_X, arr_y)\n",
        "\n",
        "    with open(\"./params/arr_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(arr_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        arr_rf_scores = cross_val_score(arr_random_forest_grided_cv, arr_X, arr_y, cv=10)\n",
        "        print(arr_rf_scores)\n",
        "        plot_confusion_matrix(arr_random_forest_grided_cv, arr_X, arr_y)\n",
        "else:\n",
        "    with open(arr_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        arr_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        arr_random_forest_grided_cv = arr_random_forest_clf.fit(arr_X, arr_y)\n",
        "\n",
        "        arr_rf_scores = cross_val_score(arr_random_forest_grided_cv, arr_X, arr_y, cv=10)\n",
        "        print(arr_rf_scores)\n",
        "        plot_confusion_matrix(arr_random_forest_grided_cv, arr_X, arr_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "arr_knn_params_file = \"./params/arr_knn_grided_cv.json\"\n",
        "arr_knn_scores = []\n",
        "\n",
        "if not exists(arr_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    arr_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    arr_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    arr_knn_grided_cv = GridSearchCV(arr_knn_clf, arr_knn_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    arr_knn_grided_cv.fit(arr_X, arr_y)\n",
        "\n",
        "    with open(\"./params/arr_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(arr_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        arr_knn_scores = cross_val_score(arr_knn_grided_cv, arr_X, arr_y, cv=10)\n",
        "        print(arr_knn_scores)\n",
        "        plot_confusion_matrix(arr_knn_grided_cv, arr_X, arr_y)\n",
        "else:\n",
        "    with open(arr_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        arr_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        arr_knn_grided_cv = arr_knn_clf.fit(arr_X, arr_y)\n",
        "\n",
        "        arr_knn_scores = cross_val_score(arr_knn_grided_cv, arr_X, arr_y, cv=10)\n",
        "        print(arr_knn_scores)\n",
        "        plot_confusion_matrix(arr_knn_grided_cv, arr_X, arr_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(arr_dt_scores, arr_rf_scores, arr_knn_scores) \n",
        "\n",
        "print(f\"p = {p}\")\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([arr_dt_scores, arr_rf_scores, arr_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "plt.ylim(0.5, 1)\n",
        "plt.title(\"Arrhythmia Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Airlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Loading and fist look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Flight</th>\n",
              "      <th>AirportFrom</th>\n",
              "      <th>AirportTo</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Time</th>\n",
              "      <th>Length</th>\n",
              "      <th>Delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'CO'</td>\n",
              "      <td>269.0</td>\n",
              "      <td>b'SFO'</td>\n",
              "      <td>b'IAH'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>15.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'US'</td>\n",
              "      <td>1558.0</td>\n",
              "      <td>b'PHX'</td>\n",
              "      <td>b'CLT'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>15.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'AA'</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>b'LAX'</td>\n",
              "      <td>b'DFW'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>20.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'AA'</td>\n",
              "      <td>2466.0</td>\n",
              "      <td>b'SFO'</td>\n",
              "      <td>b'DFW'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>20.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'AS'</td>\n",
              "      <td>108.0</td>\n",
              "      <td>b'ANC'</td>\n",
              "      <td>b'SEA'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>30.0</td>\n",
              "      <td>202.0</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Airline  Flight AirportFrom AirportTo DayOfWeek  Time  Length Delay\n",
              "0   b'CO'   269.0      b'SFO'    b'IAH'      b'3'  15.0   205.0  b'1'\n",
              "1   b'US'  1558.0      b'PHX'    b'CLT'      b'3'  15.0   222.0  b'1'\n",
              "2   b'AA'  2400.0      b'LAX'    b'DFW'      b'3'  20.0   165.0  b'1'\n",
              "3   b'AA'  2466.0      b'SFO'    b'DFW'      b'3'  20.0   195.0  b'1'\n",
              "4   b'AS'   108.0      b'ANC'    b'SEA'      b'3'  30.0   202.0  b'0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "data = arff.loadarff('./datasets/airlines.arff')\n",
        "df_airlines = pd.DataFrame(data[0])\n",
        "\n",
        "df_airlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flight</th>\n",
              "      <th>Time</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>539383.000000</td>\n",
              "      <td>539383.000000</td>\n",
              "      <td>539383.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2427.928630</td>\n",
              "      <td>802.728963</td>\n",
              "      <td>132.202007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2067.429837</td>\n",
              "      <td>278.045911</td>\n",
              "      <td>70.117016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>712.000000</td>\n",
              "      <td>565.000000</td>\n",
              "      <td>81.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1809.000000</td>\n",
              "      <td>795.000000</td>\n",
              "      <td>115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3745.000000</td>\n",
              "      <td>1035.000000</td>\n",
              "      <td>162.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7814.000000</td>\n",
              "      <td>1439.000000</td>\n",
              "      <td>655.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Flight           Time         Length\n",
              "count  539383.000000  539383.000000  539383.000000\n",
              "mean     2427.928630     802.728963     132.202007\n",
              "std      2067.429837     278.045911      70.117016\n",
              "min         1.000000      10.000000       0.000000\n",
              "25%       712.000000     565.000000      81.000000\n",
              "50%      1809.000000     795.000000     115.000000\n",
              "75%      3745.000000    1035.000000     162.000000\n",
              "max      7814.000000    1439.000000     655.000000"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_airlines.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────────── skimpy summary ──────────────────────────────────────╮\n",
              "│ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                    │\n",
              "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                    │\n",
              "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                    │\n",
              "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                    │\n",
              "│ │ Number of rows    │ 539383 │ │ object      │ 5     │                                    │\n",
              "│ │ Number of columns │ 8      │ │ float64     │ 3     │                                    │\n",
              "│ └───────────────────┴────────┘ └─────────────┴───────┘                                    │\n",
              "│ <span style=\"font-style: italic\">                                         number                                         </span>  │\n",
              "│ ┏━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━┳━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n",
              "│ ┃<span style=\"font-weight: bold\">         </span>┃<span style=\"font-weight: bold\"> missing  </span>┃<span style=\"font-weight: bold\"> complete rate  </span>┃<span style=\"font-weight: bold\"> mean  </span>┃<span style=\"font-weight: bold\"> sd   </span>┃<span style=\"font-weight: bold\"> p0 </span>┃<span style=\"font-weight: bold\"> p25 </span>┃<span style=\"font-weight: bold\"> p75  </span>┃<span style=\"font-weight: bold\"> p100 </span>┃<span style=\"font-weight: bold\"> hist   </span>┃  │\n",
              "│ ┡━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━╇━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Flight </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">             1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 2400</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">2100</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">710</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">3700</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">7800</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">█▅▃▂▁▁</span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Time   </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">             1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  800</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 280</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">10</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">560</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">1000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">1400</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> ▅███▂</span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Length </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">             1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  130</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  70</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 81</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 160</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 660</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> ██▂  </span> │  │\n",
              "│ └─────────┴──────────┴────────────────┴───────┴──────┴────┴─────┴──────┴──────┴────────┘  │\n",
              "╰─────────────────────────────────────────── End ───────────────────────────────────────────╯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "╭───────────────────────────────────── skimpy summary ──────────────────────────────────────╮\n",
              "│ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                    │\n",
              "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                    │\n",
              "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mdataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                    │\n",
              "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                    │\n",
              "│ │ Number of rows    │ 539383 │ │ object      │ 5     │                                    │\n",
              "│ │ Number of columns │ 8      │ │ float64     │ 3     │                                    │\n",
              "│ └───────────────────┴────────┘ └─────────────┴───────┘                                    │\n",
              "│ \u001b[3m                                         number                                         \u001b[0m  │\n",
              "│ ┏━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━┳━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n",
              "│ ┃\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmissing \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcomplete rate \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist  \u001b[0m\u001b[1m \u001b[0m┃  │\n",
              "│ ┡━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━╇━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n",
              "│ │ \u001b[38;5;141mFlight \u001b[0m │ \u001b[36m       0\u001b[0m │ \u001b[36m             1\u001b[0m │ \u001b[36m 2400\u001b[0m │ \u001b[36m2100\u001b[0m │ \u001b[36m 1\u001b[0m │ \u001b[36m710\u001b[0m │ \u001b[36m3700\u001b[0m │ \u001b[36m7800\u001b[0m │ \u001b[32m█▅▃▂▁▁\u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mTime   \u001b[0m │ \u001b[36m       0\u001b[0m │ \u001b[36m             1\u001b[0m │ \u001b[36m  800\u001b[0m │ \u001b[36m 280\u001b[0m │ \u001b[36m10\u001b[0m │ \u001b[36m560\u001b[0m │ \u001b[36m1000\u001b[0m │ \u001b[36m1400\u001b[0m │ \u001b[32m ▅███▂\u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mLength \u001b[0m │ \u001b[36m       0\u001b[0m │ \u001b[36m             1\u001b[0m │ \u001b[36m  130\u001b[0m │ \u001b[36m  70\u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m 81\u001b[0m │ \u001b[36m 160\u001b[0m │ \u001b[36m 660\u001b[0m │ \u001b[32m ██▂  \u001b[0m │  │\n",
              "│ └─────────┴──────────┴────────────────┴───────┴──────┴────┴─────┴──────┴──────┴────────┘  │\n",
              "╰─────────────────────────────────────────── End ───────────────────────────────────────────╯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# !pip install skimpy\n",
        "from skimpy import skim\n",
        "\n",
        "skim(df_airlines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>AirportFrom</th>\n",
              "      <th>AirportTo</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'CO'</td>\n",
              "      <td>b'SFO'</td>\n",
              "      <td>b'IAH'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'US'</td>\n",
              "      <td>b'PHX'</td>\n",
              "      <td>b'CLT'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'AA'</td>\n",
              "      <td>b'LAX'</td>\n",
              "      <td>b'DFW'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'AA'</td>\n",
              "      <td>b'SFO'</td>\n",
              "      <td>b'DFW'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'AS'</td>\n",
              "      <td>b'ANC'</td>\n",
              "      <td>b'SEA'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Airline AirportFrom AirportTo DayOfWeek Delay\n",
              "0   b'CO'      b'SFO'    b'IAH'      b'3'  b'1'\n",
              "1   b'US'      b'PHX'    b'CLT'      b'3'  b'1'\n",
              "2   b'AA'      b'LAX'    b'DFW'      b'3'  b'1'\n",
              "3   b'AA'      b'SFO'    b'DFW'      b'3'  b'1'\n",
              "4   b'AS'      b'ANC'    b'SEA'      b'3'  b'0'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "air_obj_cols = list(df_airlines.columns[df_airlines.dtypes == object])\n",
        "\n",
        "df_airlines[air_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Flight</th>\n",
              "      <th>AirportFrom</th>\n",
              "      <th>AirportTo</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Time</th>\n",
              "      <th>Length</th>\n",
              "      <th>Delay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'CO'</td>\n",
              "      <td>269.0</td>\n",
              "      <td>b'SFO'</td>\n",
              "      <td>b'IAH'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>15.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'US'</td>\n",
              "      <td>1558.0</td>\n",
              "      <td>b'PHX'</td>\n",
              "      <td>b'CLT'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>15.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'AA'</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>b'LAX'</td>\n",
              "      <td>b'DFW'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>20.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'AA'</td>\n",
              "      <td>2466.0</td>\n",
              "      <td>b'SFO'</td>\n",
              "      <td>b'DFW'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>20.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'AS'</td>\n",
              "      <td>108.0</td>\n",
              "      <td>b'ANC'</td>\n",
              "      <td>b'SEA'</td>\n",
              "      <td>b'3'</td>\n",
              "      <td>30.0</td>\n",
              "      <td>202.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Airline  Flight AirportFrom AirportTo DayOfWeek  Time  Length  Delay\n",
              "0   b'CO'   269.0      b'SFO'    b'IAH'      b'3'  15.0   205.0      1\n",
              "1   b'US'  1558.0      b'PHX'    b'CLT'      b'3'  15.0   222.0      1\n",
              "2   b'AA'  2400.0      b'LAX'    b'DFW'      b'3'  20.0   165.0      1\n",
              "3   b'AA'  2466.0      b'SFO'    b'DFW'      b'3'  20.0   195.0      1\n",
              "4   b'AS'   108.0      b'ANC'    b'SEA'      b'3'  30.0   202.0      0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# special case for target column\n",
        "air_obj_cols.remove('Delay')\n",
        "\n",
        "df_airlines['Delay'] = df_airlines['Delay'].map({b'0': 0, b'1': 1})\n",
        "\n",
        "df_airlines.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Airline has 18 uniques\n",
            "AirportFrom has 293 uniques\n",
            "AirportTo has 293 uniques\n",
            "DayOfWeek has 7 uniques\n"
          ]
        }
      ],
      "source": [
        "# printing uniques for each object column\n",
        "for col in air_obj_cols:\n",
        "    print(f\"{col} has {len(df_airlines[col].unique())} uniques\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flight</th>\n",
              "      <th>Time</th>\n",
              "      <th>Length</th>\n",
              "      <th>Delay</th>\n",
              "      <th>Airline_b'9E'</th>\n",
              "      <th>Airline_b'AA'</th>\n",
              "      <th>Airline_b'AS'</th>\n",
              "      <th>Airline_b'B6'</th>\n",
              "      <th>Airline_b'CO'</th>\n",
              "      <th>Airline_b'DL'</th>\n",
              "      <th>...</th>\n",
              "      <th>AirportTo_b'XNA'</th>\n",
              "      <th>AirportTo_b'YAK'</th>\n",
              "      <th>AirportTo_b'YUM'</th>\n",
              "      <th>DayOfWeek_b'1'</th>\n",
              "      <th>DayOfWeek_b'2'</th>\n",
              "      <th>DayOfWeek_b'3'</th>\n",
              "      <th>DayOfWeek_b'4'</th>\n",
              "      <th>DayOfWeek_b'5'</th>\n",
              "      <th>DayOfWeek_b'6'</th>\n",
              "      <th>DayOfWeek_b'7'</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>269.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1558.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2400.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2466.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>108.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>202.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 615 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Flight  Time  Length  Delay  Airline_b'9E'  Airline_b'AA'  Airline_b'AS'  \\\n",
              "0   269.0  15.0   205.0      1              0              0              0   \n",
              "1  1558.0  15.0   222.0      1              0              0              0   \n",
              "2  2400.0  20.0   165.0      1              0              1              0   \n",
              "3  2466.0  20.0   195.0      1              0              1              0   \n",
              "4   108.0  30.0   202.0      0              0              0              1   \n",
              "\n",
              "   Airline_b'B6'  Airline_b'CO'  Airline_b'DL'  ...  AirportTo_b'XNA'  \\\n",
              "0              0              1              0  ...                 0   \n",
              "1              0              0              0  ...                 0   \n",
              "2              0              0              0  ...                 0   \n",
              "3              0              0              0  ...                 0   \n",
              "4              0              0              0  ...                 0   \n",
              "\n",
              "   AirportTo_b'YAK'  AirportTo_b'YUM'  DayOfWeek_b'1'  DayOfWeek_b'2'  \\\n",
              "0                 0                 0               0               0   \n",
              "1                 0                 0               0               0   \n",
              "2                 0                 0               0               0   \n",
              "3                 0                 0               0               0   \n",
              "4                 0                 0               0               0   \n",
              "\n",
              "   DayOfWeek_b'3'  DayOfWeek_b'4'  DayOfWeek_b'5'  DayOfWeek_b'6'  \\\n",
              "0               1               0               0               0   \n",
              "1               1               0               0               0   \n",
              "2               1               0               0               0   \n",
              "3               1               0               0               0   \n",
              "4               1               0               0               0   \n",
              "\n",
              "   DayOfWeek_b'7'  \n",
              "0               0  \n",
              "1               0  \n",
              "2               0  \n",
              "3               0  \n",
              "4               0  \n",
              "\n",
              "[5 rows x 615 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "m_df_airlines = pd.get_dummies(df_airlines[air_obj_cols])\n",
        "df_airlines = pd.concat([df_airlines, m_df_airlines], axis=1)\n",
        "df_airlines = df_airlines.drop(columns=air_obj_cols)\n",
        "df_airlines.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.46% 44.54% of disbalancing\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "air_bal = df_airlines['Delay'].value_counts()[0] / df_airlines['Delay'].count()\n",
        "print(f\"{air_bal * 100:.2f}% {(1 - air_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_airlines['Delay'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flight</th>\n",
              "      <th>Time</th>\n",
              "      <th>Length</th>\n",
              "      <th>Airline_b'9E'</th>\n",
              "      <th>Airline_b'AA'</th>\n",
              "      <th>Airline_b'AS'</th>\n",
              "      <th>Airline_b'B6'</th>\n",
              "      <th>Airline_b'CO'</th>\n",
              "      <th>Airline_b'DL'</th>\n",
              "      <th>Airline_b'EV'</th>\n",
              "      <th>...</th>\n",
              "      <th>AirportTo_b'XNA'</th>\n",
              "      <th>AirportTo_b'YAK'</th>\n",
              "      <th>AirportTo_b'YUM'</th>\n",
              "      <th>DayOfWeek_b'1'</th>\n",
              "      <th>DayOfWeek_b'2'</th>\n",
              "      <th>DayOfWeek_b'3'</th>\n",
              "      <th>DayOfWeek_b'4'</th>\n",
              "      <th>DayOfWeek_b'5'</th>\n",
              "      <th>DayOfWeek_b'6'</th>\n",
              "      <th>DayOfWeek_b'7'</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.044257</td>\n",
              "      <td>-2.833090</td>\n",
              "      <td>1.038236</td>\n",
              "      <td>-0.199701</td>\n",
              "      <td>-0.304092</td>\n",
              "      <td>-0.147407</td>\n",
              "      <td>-0.186402</td>\n",
              "      <td>4.953922</td>\n",
              "      <td>-0.356891</td>\n",
              "      <td>-0.233919</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045677</td>\n",
              "      <td>-0.01028</td>\n",
              "      <td>-0.025041</td>\n",
              "      <td>-0.394906</td>\n",
              "      <td>-0.390412</td>\n",
              "      <td>2.238325</td>\n",
              "      <td>-0.451825</td>\n",
              "      <td>-0.433261</td>\n",
              "      <td>-0.350308</td>\n",
              "      <td>-0.385792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.420778</td>\n",
              "      <td>-2.833090</td>\n",
              "      <td>1.280688</td>\n",
              "      <td>-0.199701</td>\n",
              "      <td>-0.304092</td>\n",
              "      <td>-0.147407</td>\n",
              "      <td>-0.186402</td>\n",
              "      <td>-0.201860</td>\n",
              "      <td>-0.356891</td>\n",
              "      <td>-0.233919</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045677</td>\n",
              "      <td>-0.01028</td>\n",
              "      <td>-0.025041</td>\n",
              "      <td>-0.394906</td>\n",
              "      <td>-0.390412</td>\n",
              "      <td>2.238325</td>\n",
              "      <td>-0.451825</td>\n",
              "      <td>-0.433261</td>\n",
              "      <td>-0.350308</td>\n",
              "      <td>-0.385792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.013509</td>\n",
              "      <td>-2.815107</td>\n",
              "      <td>0.467761</td>\n",
              "      <td>-0.199701</td>\n",
              "      <td>3.288472</td>\n",
              "      <td>-0.147407</td>\n",
              "      <td>-0.186402</td>\n",
              "      <td>-0.201860</td>\n",
              "      <td>-0.356891</td>\n",
              "      <td>-0.233919</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045677</td>\n",
              "      <td>-0.01028</td>\n",
              "      <td>-0.025041</td>\n",
              "      <td>-0.394906</td>\n",
              "      <td>-0.390412</td>\n",
              "      <td>2.238325</td>\n",
              "      <td>-0.451825</td>\n",
              "      <td>-0.433261</td>\n",
              "      <td>-0.350308</td>\n",
              "      <td>-0.385792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.018415</td>\n",
              "      <td>-2.815107</td>\n",
              "      <td>0.895617</td>\n",
              "      <td>-0.199701</td>\n",
              "      <td>3.288472</td>\n",
              "      <td>-0.147407</td>\n",
              "      <td>-0.186402</td>\n",
              "      <td>-0.201860</td>\n",
              "      <td>-0.356891</td>\n",
              "      <td>-0.233919</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045677</td>\n",
              "      <td>-0.01028</td>\n",
              "      <td>-0.025041</td>\n",
              "      <td>-0.394906</td>\n",
              "      <td>-0.390412</td>\n",
              "      <td>2.238325</td>\n",
              "      <td>-0.451825</td>\n",
              "      <td>-0.433261</td>\n",
              "      <td>-0.350308</td>\n",
              "      <td>-0.385792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.122132</td>\n",
              "      <td>-2.779142</td>\n",
              "      <td>0.995450</td>\n",
              "      <td>-0.199701</td>\n",
              "      <td>-0.304092</td>\n",
              "      <td>6.783904</td>\n",
              "      <td>-0.186402</td>\n",
              "      <td>-0.201860</td>\n",
              "      <td>-0.356891</td>\n",
              "      <td>-0.233919</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045677</td>\n",
              "      <td>-0.01028</td>\n",
              "      <td>-0.025041</td>\n",
              "      <td>-0.394906</td>\n",
              "      <td>-0.390412</td>\n",
              "      <td>2.238325</td>\n",
              "      <td>-0.451825</td>\n",
              "      <td>-0.433261</td>\n",
              "      <td>-0.350308</td>\n",
              "      <td>-0.385792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 614 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Flight      Time    Length  Airline_b'9E'  Airline_b'AA'  Airline_b'AS'  \\\n",
              "0 -1.044257 -2.833090  1.038236      -0.199701      -0.304092      -0.147407   \n",
              "1 -0.420778 -2.833090  1.280688      -0.199701      -0.304092      -0.147407   \n",
              "2 -0.013509 -2.815107  0.467761      -0.199701       3.288472      -0.147407   \n",
              "3  0.018415 -2.815107  0.895617      -0.199701       3.288472      -0.147407   \n",
              "4 -1.122132 -2.779142  0.995450      -0.199701      -0.304092       6.783904   \n",
              "\n",
              "   Airline_b'B6'  Airline_b'CO'  Airline_b'DL'  Airline_b'EV'  ...  \\\n",
              "0      -0.186402       4.953922      -0.356891      -0.233919  ...   \n",
              "1      -0.186402      -0.201860      -0.356891      -0.233919  ...   \n",
              "2      -0.186402      -0.201860      -0.356891      -0.233919  ...   \n",
              "3      -0.186402      -0.201860      -0.356891      -0.233919  ...   \n",
              "4      -0.186402      -0.201860      -0.356891      -0.233919  ...   \n",
              "\n",
              "   AirportTo_b'XNA'  AirportTo_b'YAK'  AirportTo_b'YUM'  DayOfWeek_b'1'  \\\n",
              "0         -0.045677          -0.01028         -0.025041       -0.394906   \n",
              "1         -0.045677          -0.01028         -0.025041       -0.394906   \n",
              "2         -0.045677          -0.01028         -0.025041       -0.394906   \n",
              "3         -0.045677          -0.01028         -0.025041       -0.394906   \n",
              "4         -0.045677          -0.01028         -0.025041       -0.394906   \n",
              "\n",
              "   DayOfWeek_b'2'  DayOfWeek_b'3'  DayOfWeek_b'4'  DayOfWeek_b'5'  \\\n",
              "0       -0.390412        2.238325       -0.451825       -0.433261   \n",
              "1       -0.390412        2.238325       -0.451825       -0.433261   \n",
              "2       -0.390412        2.238325       -0.451825       -0.433261   \n",
              "3       -0.390412        2.238325       -0.451825       -0.433261   \n",
              "4       -0.390412        2.238325       -0.451825       -0.433261   \n",
              "\n",
              "   DayOfWeek_b'6'  DayOfWeek_b'7'  \n",
              "0       -0.350308       -0.385792  \n",
              "1       -0.350308       -0.385792  \n",
              "2       -0.350308       -0.385792  \n",
              "3       -0.350308       -0.385792  \n",
              "4       -0.350308       -0.385792  \n",
              "\n",
              "[5 rows x 614 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "air_X = df_airlines.drop(columns=['Delay'])\n",
        "air_y = df_airlines['Delay']\n",
        "\n",
        "air_X = (air_X - air_X.mean()) / air_X.std()\n",
        "\n",
        "air_X = air_X.dropna(axis='columns')\n",
        "\n",
        "air_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "air_dt_params_file = \"./params/air_decision_tree_grided_cv.json\"\n",
        "air_dt_scores = []\n",
        "\n",
        "if not exists(air_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    air_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    air_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    air_decision_tree_grided_cv = RandomizedSearchCV(air_decision_tree_clf, air_tree_param, cv=10, verbose=10, n_jobs=-1, pre_dispatch=2)\n",
        "    # fitting to best params\n",
        "    air_decision_tree_grided_cv.fit(air_X, air_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(air_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(air_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        air_dt_scores = cross_val_score(air_decision_tree_grided_cv, air_X, air_y, cv=10)\n",
        "        print(air_dt_scores)\n",
        "        plot_confusion_matrix(air_decision_tree_grided_cv, air_X, air_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(air_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        air_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        air_decision_tree_grided_cv = air_decision_tree_clf.fit(air_X, air_y)\n",
        "\n",
        "        air_dt_scores = cross_val_score(air_decision_tree_grided_cv, air_X, air_y, cv=10)\n",
        "        print(air_dt_scores)\n",
        "        plot_confusion_matrix(air_decision_tree_grided_cv, air_X, air_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "air_rf_params_file = \"./params/air_random_forest_grided_cv.json\"\n",
        "air_rf_scores = []\n",
        "\n",
        "if not exists(air_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    air_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    air_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    air_random_forest_grided_cv = RandomizedSearchCV(air_random_forest_clf, air_tree_param, cv=10, verbose=10, n_jobs=-1, pre_dispatch=2)\n",
        "    # fitting to best params\n",
        "    air_random_forest_grided_cv.fit(air_X, air_y)\n",
        "\n",
        "    with open(\"./params/air_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(air_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        air_rf_scores = cross_val_score(air_random_forest_grided_cv, air_X, air_y, cv=10)\n",
        "        print(air_rf_scores)\n",
        "        plot_confusion_matrix(air_random_forest_grided_cv, air_X, air_y)\n",
        "else:\n",
        "    with open(air_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        air_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        air_random_forest_grided_cv = air_random_forest_clf.fit(air_X, air_y)\n",
        "\n",
        "        air_rf_scores = cross_val_score(air_random_forest_grided_cv, air_X, air_y, cv=10)\n",
        "        print(air_rf_scores)\n",
        "        plot_confusion_matrix(air_random_forest_grided_cv, air_X, air_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "air_knn_params_file = \"./params/air_knn_grided_cv.json\"\n",
        "air_knn_scores = []\n",
        "\n",
        "if not exists(air_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    air_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    air_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    air_knn_grided_cv = RandomizedSearchCV(air_knn_clf, air_knn_param, cv=10, verbose=10, n_jobs=-1, pre_dispatch=2)\n",
        "    # fitting to best params\n",
        "    air_knn_grided_cv.fit(air_X, air_y)\n",
        "\n",
        "    with open(\"./params/air_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(air_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        air_knn_scores = cross_val_score(air_knn_grided_cv, air_X, air_y, cv=10)\n",
        "        print(air_knn_scores)\n",
        "        plot_confusion_matrix(air_knn_grided_cv, air_X, air_y)\n",
        "else:\n",
        "    with open(air_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        air_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        air_knn_grided_cv = air_knn_clf.fit(air_X, air_y)\n",
        "\n",
        "        air_knn_scores = cross_val_score(air_knn_grided_cv, air_X, air_y, cv=10)\n",
        "        print(air_knn_scores)\n",
        "        plot_confusion_matrix(air_knn_grided_cv, air_X, air_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(air_dt_scores, air_rf_scores, air_knn_scores) \n",
        "\n",
        "print(f\"p = {p}\")\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([air_dt_scores, air_rf_scores, air_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.title(\"Airlines Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phoneme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Loading and first look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "data = arff.loadarff('./datasets/phonemes.arff')\n",
        "df_phonemes = pd.DataFrame(data[0])\n",
        "\n",
        "df_phonemes.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_phonemes.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install skimpy\n",
        "from skimpy import skim\n",
        "\n",
        "skim(df_phonemes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pho_obj_cols = list(df_phonemes.columns[df_phonemes.dtypes == object])\n",
        "\n",
        "df_phonemes[pho_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_phonemes['Class'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# special case for target column\n",
        "pho_obj_cols.remove('Class')\n",
        "\n",
        "df_phonemes['Class'] = df_phonemes['Class'].map({b'1': 0, b'2': 1})\n",
        "\n",
        "df_phonemes.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pho_bal = df_phonemes['Class'].value_counts()[0] / df_phonemes['Class'].count()\n",
        "print(f\"{pho_bal * 100:.2f}% {(1 - pho_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_phonemes['Class'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "\n",
        "df_phonemes_target = df_phonemes['Class']\n",
        "df_phonemes = df_phonemes.drop(columns=['Class'])\n",
        "df_phonemes, df_phonemes_target = ros.fit_resample(df_phonemes, df_phonemes_target)\n",
        "\n",
        "df_phonemes = pd.concat([df_phonemes, df_phonemes_target], axis=1)\n",
        "\n",
        "pho_bal = df_phonemes['Class'].value_counts()[0] / df_phonemes['Class'].count()\n",
        "print(f\"{pho_bal * 100:.2f}% {(1 - pho_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_phonemes['Class'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pho_X = df_phonemes.drop(columns=['Class'])\n",
        "pho_y = df_phonemes['Class']\n",
        "\n",
        "pho_X = (pho_X - pho_X.mean()) / pho_X.std()\n",
        "\n",
        "pho_X = pho_X.dropna(axis='columns')\n",
        "\n",
        "pho_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "pho_dt_params_file = \"./params/pho_decision_tree_grided_cv.json\"\n",
        "pho_dt_scores = []\n",
        "\n",
        "if not exists(pho_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    pho_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    pho_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    pho_decision_tree_grided_cv = GridSearchCV(pho_decision_tree_clf, pho_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    pho_decision_tree_grided_cv.fit(pho_X, pho_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(pho_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(pho_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        pho_dt_scores = cross_val_score(pho_decision_tree_grided_cv, pho_X, pho_y, cv=10)\n",
        "        print(pho_dt_scores)\n",
        "        plot_confusion_matrix(pho_decision_tree_grided_cv, pho_X, pho_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(pho_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        pho_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        pho_decision_tree_grided_cv = pho_decision_tree_clf.fit(pho_X, pho_y)\n",
        "\n",
        "        pho_dt_scores = cross_val_score(pho_decision_tree_grided_cv, pho_X, pho_y, cv=10)\n",
        "        print(pho_dt_scores)\n",
        "        plot_confusion_matrix(pho_decision_tree_grided_cv, pho_X, pho_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "pho_rf_params_file = \"./params/pho_random_forest_grided_cv.json\"\n",
        "pho_rf_scores = []\n",
        "\n",
        "if not exists(pho_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    pho_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    pho_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    pho_random_forest_grided_cv = GridSearchCV(pho_random_forest_clf, pho_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    pho_random_forest_grided_cv.fit(pho_X, pho_y)\n",
        "\n",
        "    with open(\"./params/pho_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(pho_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        pho_rf_scores = cross_val_score(pho_random_forest_grided_cv, pho_X, pho_y, cv=10)\n",
        "        print(pho_rf_scores)\n",
        "        plot_confusion_matrix(pho_random_forest_grided_cv, pho_X, pho_y)\n",
        "else:\n",
        "    with open(pho_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        pho_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        pho_random_forest_grided_cv = pho_random_forest_clf.fit(pho_X, pho_y)\n",
        "\n",
        "        pho_rf_scores = cross_val_score(pho_random_forest_grided_cv, pho_X, pho_y, cv=10)\n",
        "        print(pho_rf_scores)\n",
        "        plot_confusion_matrix(pho_random_forest_grided_cv, pho_X, pho_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "pho_knn_params_file = \"./params/pho_knn_grided_cv.json\"\n",
        "pho_knn_scores = []\n",
        "\n",
        "if not exists(pho_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    pho_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    pho_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    pho_knn_grided_cv = GridSearchCV(pho_knn_clf, pho_knn_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    pho_knn_grided_cv.fit(pho_X, pho_y)\n",
        "\n",
        "    with open(\"./params/pho_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(pho_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        pho_knn_scores = cross_val_score(pho_knn_grided_cv, pho_X, pho_y, cv=10)\n",
        "        print(pho_knn_scores)\n",
        "        plot_confusion_matrix(pho_knn_grided_cv, pho_X, pho_y)\n",
        "else:\n",
        "    with open(pho_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        pho_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        pho_knn_grided_cv = pho_knn_clf.fit(pho_X, pho_y)\n",
        "\n",
        "        pho_knn_scores = cross_val_score(pho_knn_grided_cv, pho_X, pho_y, cv=10)\n",
        "        print(pho_knn_scores)\n",
        "        plot_confusion_matrix(pho_knn_grided_cv, pho_X, pho_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(pho_dt_scores, pho_rf_scores, pho_knn_scores) \n",
        "\n",
        "print(f\"p = {p}\")\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([pho_dt_scores, pho_rf_scores, pho_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "plt.ylim(0.8, 1)\n",
        "plt.title(\"Phoneme Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phishing Websites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset loading and first look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "data = arff.loadarff('./datasets/phishing.arff')\n",
        "df_phishing = pd.DataFrame(data[0])\n",
        "\n",
        "df_phishing.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_phishing.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "phi_obj_cols = list(df_phishing.columns[df_phishing.dtypes == object])\n",
        "\n",
        "df_phishing[phi_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "phi_unary_cols = []\n",
        "\n",
        "for col in phi_obj_cols:\n",
        "    curr_col_uniques = len(df_phishing[col].unique())\n",
        "    # if is a binary column\n",
        "    if curr_col_uniques == 2 or curr_col_uniques == 3:\n",
        "        bin_values = df_phishing[col].unique()\n",
        "        df_phishing[col] = df_phishing[col].map(lambda v: int(v))\n",
        "    # if column is unary\n",
        "    elif len(df_phishing[col].unique()) == 1:\n",
        "        phi_unary_cols.append(col)\n",
        "    # print problematic column\n",
        "    else:\n",
        "        print(col, len(df_phishing[col].unique()))\n",
        "\n",
        "# removing unary cols\n",
        "df_phishing = df_phishing.drop(columns=phi_unary_cols)\n",
        "\n",
        "phi_obj_cols = [col for col in phi_obj_cols if col not in phi_unary_cols]\n",
        "df_phishing[phi_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "phi_bal = df_phishing['Result'].value_counts()[1] / df_phishing['Result'].count()\n",
        "print(f\"{phi_bal * 100:.2f}% {(1 - phi_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_phishing['Result'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "phi_X = df_phishing.drop(columns=['Result'])\n",
        "phi_y = df_phishing['Result']\n",
        "\n",
        "phi_X = (phi_X - phi_X.mean()) / phi_X.std()\n",
        "\n",
        "phi_X = phi_X.dropna(axis='columns')\n",
        "\n",
        "phi_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "phi_dt_params_file = \"./params/phi_decision_tree_grided_cv.json\"\n",
        "phi_dt_scores = []\n",
        "\n",
        "if not exists(phi_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    phi_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    phi_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    phi_decision_tree_grided_cv = GridSearchCV(phi_decision_tree_clf, phi_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    phi_decision_tree_grided_cv.fit(phi_X, phi_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(phi_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(phi_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        phi_dt_scores = cross_val_score(phi_decision_tree_grided_cv, phi_X, phi_y, cv=10)\n",
        "        print(phi_dt_scores)\n",
        "        plot_confusion_matrix(phi_decision_tree_grided_cv, phi_X, phi_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(phi_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        phi_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        phi_decision_tree_grided_cv = phi_decision_tree_clf.fit(phi_X, phi_y)\n",
        "\n",
        "        phi_dt_scores = cross_val_score(phi_decision_tree_grided_cv, phi_X, phi_y, cv=10)\n",
        "        print(phi_dt_scores)\n",
        "        plot_confusion_matrix(phi_decision_tree_grided_cv, phi_X, phi_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "phi_rf_params_file = \"./params/phi_random_forest_grided_cv.json\"\n",
        "phi_rf_scores = []\n",
        "\n",
        "if not exists(phi_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    phi_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    phi_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    phi_random_forest_grided_cv = GridSearchCV(phi_random_forest_clf, phi_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    phi_random_forest_grided_cv.fit(phi_X, phi_y)\n",
        "\n",
        "    with open(\"./params/phi_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(phi_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        phi_rf_scores = cross_val_score(phi_random_forest_grided_cv, phi_X, phi_y, cv=10)\n",
        "        print(phi_rf_scores)\n",
        "        plot_confusion_matrix(phi_random_forest_grided_cv, phi_X, phi_y)\n",
        "else:\n",
        "    with open(phi_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        phi_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        phi_random_forest_grided_cv = phi_random_forest_clf.fit(phi_X, phi_y)\n",
        "\n",
        "        phi_rf_scores = cross_val_score(phi_random_forest_grided_cv, phi_X, phi_y, cv=10)\n",
        "        print(phi_rf_scores)\n",
        "        plot_confusion_matrix(phi_random_forest_grided_cv, phi_X, phi_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "phi_knn_params_file = \"./params/phi_knn_grided_cv.json\"\n",
        "phi_knn_scores = []\n",
        "\n",
        "if not exists(phi_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    phi_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    phi_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    phi_knn_grided_cv = GridSearchCV(phi_knn_clf, phi_knn_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    phi_knn_grided_cv.fit(phi_X, phi_y)\n",
        "\n",
        "    with open(\"./params/phi_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(phi_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        phi_knn_scores = cross_val_score(phi_knn_grided_cv, phi_X, phi_y, cv=10)\n",
        "        print(phi_knn_scores)\n",
        "        plot_confusion_matrix(phi_knn_grided_cv, phi_X, phi_y)\n",
        "else:\n",
        "    with open(phi_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        phi_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        phi_knn_grided_cv = phi_knn_clf.fit(phi_X, phi_y)\n",
        "\n",
        "        phi_knn_scores = cross_val_score(phi_knn_grided_cv, phi_X, phi_y, cv=10)\n",
        "        print(phi_knn_scores)\n",
        "        plot_confusion_matrix(phi_knn_grided_cv, phi_X, phi_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(phi_dt_scores, phi_rf_scores, phi_knn_scores) \n",
        "\n",
        "print(f\"p = {p}\")\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([phi_dt_scores, phi_rf_scores, phi_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.title(\"Phishing Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Satellite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset loading and first look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "data = arff.loadarff('./datasets/satellite.arff')\n",
        "df_satellite = pd.DataFrame(data[0])\n",
        "\n",
        "df_satellite.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_satellite.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install skimpy\n",
        "from skimpy import skim\n",
        "\n",
        "skim(df_satellite)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sat_obj_cols = list(df_satellite.columns[df_satellite.dtypes == object])\n",
        "\n",
        "df_satellite[sat_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_satellite['Target'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# special case for target column\n",
        "sat_obj_cols.remove('Target')\n",
        "\n",
        "df_satellite['Target'] = df_satellite['Target'].map({b'Anomaly': 0, b'Normal': 1})\n",
        "\n",
        "df_satellite.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sat_bal = df_satellite['Target'].value_counts()[1] / df_satellite['Target'].count()\n",
        "print(f\"{sat_bal * 100:.2f}% {(1 - sat_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_satellite['Target'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install imblearn\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "smote_tmk = SMOTETomek(sampling_strategy=0.1)\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "\n",
        "df_satellite_target = df_satellite['Target']\n",
        "df_satellite = df_satellite.drop(columns=['Target'])\n",
        "df_satellite, df_satellite_target = smote_tmk.fit_resample(df_satellite, df_satellite_target)\n",
        "df_satellite, df_satellite_target = rus.fit_resample(df_satellite, df_satellite_target)\n",
        "\n",
        "df_satellite = pd.concat([df_satellite, df_satellite_target], axis=1)\n",
        "\n",
        "sat_bal = df_satellite['Target'].value_counts()[0] / df_satellite['Target'].count()\n",
        "print(f\"counts:\\n{df_satellite['Target'].value_counts()}\")\n",
        "print(f\"{sat_bal * 100:.2f}% {(1 - sat_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_satellite['Target'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sat_X = df_satellite.drop(columns=['Target'])\n",
        "sat_y = df_satellite['Target']\n",
        "\n",
        "sat_X = (sat_X - sat_X.mean()) / sat_X.std()\n",
        "\n",
        "sat_X = sat_X.dropna(axis='columns')\n",
        "\n",
        "sat_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "sat_dt_params_file = \"./params/sat_decision_tree_grided_cv.json\"\n",
        "sat_dt_scores = []\n",
        "\n",
        "if not exists(sat_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    sat_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    sat_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    sat_decision_tree_grided_cv = GridSearchCV(sat_decision_tree_clf, sat_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    sat_decision_tree_grided_cv.fit(sat_X, sat_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(sat_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(sat_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        sat_dt_scores = cross_val_score(sat_decision_tree_grided_cv, sat_X, sat_y, cv=10)\n",
        "        print(sat_dt_scores)\n",
        "        plot_confusion_matrix(sat_decision_tree_grided_cv, sat_X, sat_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(sat_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        sat_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        sat_decision_tree_grided_cv = sat_decision_tree_clf.fit(sat_X, sat_y)\n",
        "\n",
        "        sat_dt_scores = cross_val_score(sat_decision_tree_grided_cv, sat_X, sat_y, cv=10)\n",
        "        print(sat_dt_scores)\n",
        "        plot_confusion_matrix(sat_decision_tree_grided_cv, sat_X, sat_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "sat_rf_params_file = \"./params/sat_random_forest_grided_cv.json\"\n",
        "sat_rf_scores = []\n",
        "\n",
        "if not exists(sat_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    sat_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    sat_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    sat_random_forest_grided_cv = GridSearchCV(sat_random_forest_clf, sat_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    sat_random_forest_grided_cv.fit(sat_X, sat_y)\n",
        "\n",
        "    with open(\"./params/sat_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(sat_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        sat_rf_scores = cross_val_score(sat_random_forest_grided_cv, sat_X, sat_y, cv=10)\n",
        "        print(sat_rf_scores)\n",
        "        plot_confusion_matrix(sat_random_forest_grided_cv, sat_X, sat_y)\n",
        "else:\n",
        "    with open(sat_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        sat_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        sat_random_forest_grided_cv = sat_random_forest_clf.fit(sat_X, sat_y)\n",
        "\n",
        "        sat_rf_scores = cross_val_score(sat_random_forest_grided_cv, sat_X, sat_y, cv=10)\n",
        "        print(sat_rf_scores)\n",
        "        plot_confusion_matrix(sat_random_forest_grided_cv, sat_X, sat_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "sat_knn_params_file = \"./params/sat_knn_grided_cv.json\"\n",
        "sat_knn_scores = []\n",
        "\n",
        "if not exists(sat_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    sat_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    sat_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    sat_knn_grided_cv = GridSearchCV(sat_knn_clf, sat_knn_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    sat_knn_grided_cv.fit(sat_X, sat_y)\n",
        "\n",
        "    with open(\"./params/sat_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(sat_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        sat_knn_scores = cross_val_score(sat_knn_grided_cv, sat_X, sat_y, cv=10)\n",
        "        print(sat_knn_scores)\n",
        "        plot_confusion_matrix(sat_knn_grided_cv, sat_X, sat_y)\n",
        "else:\n",
        "    with open(sat_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        sat_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        sat_knn_grided_cv = sat_knn_clf.fit(sat_X, sat_y)\n",
        "\n",
        "        sat_knn_scores = cross_val_score(sat_knn_grided_cv, sat_X, sat_y, cv=10)\n",
        "        print(sat_knn_scores)\n",
        "        plot_confusion_matrix(sat_knn_grided_cv, sat_X, sat_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(sat_dt_scores, sat_rf_scores, sat_knn_scores) \n",
        "\n",
        "print(f\"p = {p}\")\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([sat_dt_scores, sat_rf_scores, sat_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.title(\"Satellite Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset loading and first look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "data = arff.loadarff('./datasets/adult.arff')\n",
        "df_adult = pd.DataFrame(data[0])\n",
        "\n",
        "df_adult.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_adult.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install skimpy\n",
        "from skimpy import skim\n",
        "\n",
        "skim(df_adult)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adu_obj_cols = list(df_adult.columns[df_adult.dtypes == object])\n",
        "\n",
        "df_adult[adu_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# printing uniques for each object column\n",
        "for col in adu_obj_cols:\n",
        "    print(f\"{col} has {len(df_adult[col].unique())} uniques\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adu_unary_cols = []\n",
        "\n",
        "for col in adu_obj_cols:\n",
        "    curr_col_uniques = len(df_adult[col].unique())\n",
        "    # if is a binary column\n",
        "    if curr_col_uniques == 2:\n",
        "        bin_values = list(df_adult[col].unique())\n",
        "        df_adult[col] = df_adult[col].map({bin_values[0]: 0, bin_values[1]: 1})\n",
        "    # if column is unary\n",
        "    elif len(df_adult[col].unique()) == 1:\n",
        "        # add unary column to list\n",
        "        adu_unary_cols.append(col)\n",
        "    # print one hot encode column\n",
        "    else:\n",
        "        # get dummies of current column\n",
        "        m_df_adult = pd.get_dummies(df_adult[col], prefix=col)\n",
        "        # concat to original dataframe\n",
        "        df_adult = pd.concat([df_adult, m_df_adult], axis=1)\n",
        "        # drop current column from original dataframe\n",
        "        df_adult = df_adult.drop(columns=[col])\n",
        "\n",
        "# removing unary cols\n",
        "df_adult = df_adult.drop(columns=adu_unary_cols)\n",
        "\n",
        "adu_obj_cols = [col for col in adu_obj_cols if col not in adu_unary_cols]\n",
        "df_adult.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adu_bal = df_adult['class'].value_counts()[1] / df_adult['class'].count()\n",
        "print(f\"{adu_bal * 100:.2f}% {(1 - adu_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_adult['class'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install imblearn\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "smote_tmk = SMOTETomek(sampling_strategy=0.35)\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "\n",
        "df_adult_target = df_adult['class']\n",
        "df_adult = df_adult.drop(columns=['class'])\n",
        "# smote oversampling of minority class\n",
        "df_adult, df_adult_target = smote_tmk.fit_resample(df_adult, df_adult_target)\n",
        "# random downsampling of majority class\n",
        "df_adult, df_adult_target = rus.fit_resample(df_adult, df_adult_target)\n",
        "\n",
        "df_adult = pd.concat([df_adult, df_adult_target], axis=1)\n",
        "\n",
        "adu_bal = df_adult['class'].value_counts()[0] / df_adult['class'].count()\n",
        "print(f\"counts:\\n{df_adult['class'].value_counts()}\")\n",
        "print(f\"{adu_bal * 100:.2f}% {(1 - adu_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_adult['class'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adu_X = df_adult.drop(columns=['class'])\n",
        "adu_y = df_adult['class']\n",
        "\n",
        "adu_X = (adu_X - adu_X.mean()) / adu_X.std()\n",
        "\n",
        "adu_X = adu_X.dropna(axis='columns')\n",
        "\n",
        "adu_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "adu_dt_params_file = \"./params/adu_decision_tree_grided_cv.json\"\n",
        "adu_dt_scores = []\n",
        "\n",
        "if not exists(adu_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    adu_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    adu_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    adu_decision_tree_grided_cv = GridSearchCV(adu_decision_tree_clf, adu_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    adu_decision_tree_grided_cv.fit(adu_X, adu_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(adu_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(adu_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        adu_dt_scores = cross_val_score(adu_decision_tree_grided_cv, adu_X, adu_y, cv=10)\n",
        "        print(adu_dt_scores)\n",
        "        plot_confusion_matrix(adu_decision_tree_grided_cv, adu_X, adu_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(adu_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        adu_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        adu_decision_tree_grided_cv = adu_decision_tree_clf.fit(adu_X, adu_y)\n",
        "\n",
        "        adu_dt_scores = cross_val_score(adu_decision_tree_grided_cv, adu_X, adu_y, cv=10)\n",
        "        print(adu_dt_scores)\n",
        "        plot_confusion_matrix(adu_decision_tree_grided_cv, adu_X, adu_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "adu_rf_params_file = \"./params/adu_random_forest_grided_cv.json\"\n",
        "adu_rf_scores = []\n",
        "\n",
        "if not exists(adu_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    adu_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    adu_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    adu_random_forest_grided_cv = GridSearchCV(adu_random_forest_clf, adu_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    adu_random_forest_grided_cv.fit(adu_X, adu_y)\n",
        "\n",
        "    with open(\"./params/adu_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(adu_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        adu_rf_scores = cross_val_score(adu_random_forest_grided_cv, adu_X, adu_y, cv=10)\n",
        "        print(adu_rf_scores)\n",
        "        plot_confusion_matrix(adu_random_forest_grided_cv, adu_X, adu_y)\n",
        "else:\n",
        "    with open(adu_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        adu_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        adu_random_forest_grided_cv = adu_random_forest_clf.fit(adu_X, adu_y)\n",
        "\n",
        "        adu_rf_scores = cross_val_score(adu_random_forest_grided_cv, adu_X, adu_y, cv=10)\n",
        "        print(adu_rf_scores)\n",
        "        plot_confusion_matrix(adu_random_forest_grided_cv, adu_X, adu_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "adu_knn_params_file = \"./params/adu_knn_grided_cv.json\"\n",
        "adu_knn_scores = []\n",
        "\n",
        "if not exists(adu_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    adu_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    adu_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    adu_knn_grided_cv = GridSearchCV(adu_knn_clf, adu_knn_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    adu_knn_grided_cv.fit(adu_X, adu_y)\n",
        "\n",
        "    with open(\"./params/adu_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(adu_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        adu_knn_scores = cross_val_score(adu_knn_grided_cv, adu_X, adu_y, cv=10)\n",
        "        print(adu_knn_scores)\n",
        "        plot_confusion_matrix(adu_knn_grided_cv, adu_X, adu_y)\n",
        "else:\n",
        "    with open(adu_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        adu_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        adu_knn_grided_cv = adu_knn_clf.fit(adu_X, adu_y)\n",
        "\n",
        "        adu_knn_scores = cross_val_score(adu_knn_grided_cv, adu_X, adu_y, cv=10)\n",
        "        print(adu_knn_scores)\n",
        "        plot_confusion_matrix(adu_knn_grided_cv, adu_X, adu_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(adu_dt_scores, adu_rf_scores, adu_knn_scores) \n",
        "\n",
        "print(f\"p = {p}\")\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([adu_dt_scores, adu_rf_scores, adu_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.title(\"Adult Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AedesSex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset loading and first look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_aedes = pd.read_csv('./datasets/aedessex.csv')\n",
        "\n",
        "df_aedes.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_aedes.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Skimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install skimpy\n",
        "from skimpy import skim\n",
        "\n",
        "skim(df_aedes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### object columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aed_obj_cols = list(df_aedes.columns[df_aedes.dtypes == object])\n",
        "\n",
        "df_aedes[aed_obj_cols].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_aedes['sex'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# binarize target class\n",
        "\n",
        "df_aedes['sex'] = df_aedes['sex'].map({'F': 0, 'M': 1})\n",
        "df_aedes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Target class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aed_bal = df_aedes['sex'].value_counts()[1] / df_aedes['sex'].count()\n",
        "print(f\"{aed_bal * 100:.2f}% {(1 - aed_bal) * 100:.2f}% of disbalancing\")\n",
        "\n",
        "df_aedes['sex'].value_counts().plot.bar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aed_X = df_aedes.drop(columns=['sex'])\n",
        "aed_y = df_aedes['sex']\n",
        "\n",
        "aed_X = (aed_X - aed_X.mean()) / aed_X.std()\n",
        "\n",
        "aed_X = aed_X.dropna(axis='columns')\n",
        "\n",
        "aed_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# aed_X_train, aed_X_test, aed_y_train, aed_y_test = train_test_split(aed_X, aed_y, test_size=0.33, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "aed_dt_params_file = \"./params/aed_decision_tree_grided_cv.json\"\n",
        "aed_dt_scores = []\n",
        "\n",
        "if not exists(aed_dt_params_file):\n",
        "    # instantiating classifier\n",
        "    aed_decision_tree_clf = DecisionTreeClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    aed_tree_param = {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]\n",
        "    }\n",
        "    # training on each param combination\n",
        "    aed_decision_tree_grided_cv = GridSearchCV(aed_decision_tree_clf, aed_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    aed_decision_tree_grided_cv.fit(aed_X, aed_y)\n",
        "    # write file with parameters to save on computation next run\n",
        "    with open(aed_dt_params_file, \"w+\") as param_f:\n",
        "        json.dump(aed_decision_tree_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        aed_dt_scores = cross_val_score(aed_decision_tree_grided_cv, aed_X, aed_y, cv=10)\n",
        "        print(aed_dt_scores)\n",
        "        plot_confusion_matrix(aed_decision_tree_grided_cv, aed_X, aed_y)\n",
        "else:\n",
        "    # read params file\n",
        "    with open(aed_dt_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        aed_decision_tree_clf = DecisionTreeClassifier(random_state=0, **json.load(param_f))\n",
        "        aed_decision_tree_grided_cv = aed_decision_tree_clf.fit(aed_X, aed_y)\n",
        "\n",
        "        aed_dt_scores = cross_val_score(aed_decision_tree_grided_cv, aed_X, aed_y, cv=10)\n",
        "        print(aed_dt_scores)\n",
        "        plot_confusion_matrix(aed_decision_tree_grided_cv, aed_X, aed_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "aed_rf_params_file = \"./params/aed_random_forest_grided_cv.json\"\n",
        "aed_rf_scores = []\n",
        "\n",
        "if not exists(aed_rf_params_file):\n",
        "    # instantiating classifier\n",
        "    aed_random_forest_clf = RandomForestClassifier(random_state=0)\n",
        "    # defining testing params\n",
        "    aed_tree_param = {\n",
        "        'bootstrap': [True, False],\n",
        "        'max_depth': [10, 20, None], # 30, 40, 50, 60, 70, 80, 90, 100, \n",
        "        'max_features': ['sqrt'],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'n_estimators': [200, 400, 600] #, 800, 1000, 1200, 1400, 1600, 1800, 2000\n",
        "    }\n",
        "    # training on each param combination\n",
        "    aed_random_forest_grided_cv = GridSearchCV(aed_random_forest_clf, aed_tree_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    aed_random_forest_grided_cv.fit(aed_X, aed_y)\n",
        "\n",
        "    with open(\"./params/aed_random_forest_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(aed_random_forest_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        aed_rf_scores = cross_val_score(aed_random_forest_grided_cv, aed_X, aed_y, cv=10)\n",
        "        print(aed_rf_scores)\n",
        "        plot_confusion_matrix(aed_random_forest_grided_cv, aed_X, aed_y)\n",
        "else:\n",
        "    with open(aed_rf_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        aed_random_forest_clf = RandomForestClassifier(random_state=0, **json.load(param_f))\n",
        "        aed_random_forest_grided_cv = aed_random_forest_clf.fit(aed_X, aed_y)\n",
        "\n",
        "        aed_rf_scores = cross_val_score(aed_random_forest_grided_cv, aed_X, aed_y, cv=10)\n",
        "        print(aed_rf_scores)\n",
        "        plot_confusion_matrix(aed_random_forest_grided_cv, aed_X, aed_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from os.path import exists\n",
        "import json\n",
        "\n",
        "aed_knn_params_file = \"./params/aed_knn_grided_cv.json\"\n",
        "aed_knn_scores = []\n",
        "\n",
        "if not exists(aed_knn_params_file):\n",
        "    # instantiating classifier\n",
        "    aed_knn_clf = KNeighborsClassifier()\n",
        "    # defining testing params\n",
        "    aed_knn_param = dict(n_neighbors=list(range(1, 31)))\n",
        "    # training on each param combination\n",
        "    aed_knn_grided_cv = GridSearchCV(aed_knn_clf, aed_knn_param, cv=10, verbose=10, n_jobs=-1)\n",
        "    # fitting to best params\n",
        "    aed_knn_grided_cv.fit(aed_X, aed_y)\n",
        "\n",
        "    with open(\"./params/aed_knn_grided_cv.json\", \"w\") as param_f:\n",
        "        json.dump(aed_knn_grided_cv.best_params_, param_f, indent=4)\n",
        "\n",
        "        aed_knn_scores = cross_val_score(aed_knn_grided_cv, aed_X, aed_y, cv=10)\n",
        "        print(aed_knn_scores)\n",
        "        plot_confusion_matrix(aed_knn_grided_cv, aed_X, aed_y)\n",
        "else:\n",
        "    with open(aed_knn_params_file, \"r\") as param_f:\n",
        "        # instantiating classifier\n",
        "        aed_knn_clf = KNeighborsClassifier(**json.load(param_f))\n",
        "        aed_knn_grided_cv = aed_knn_clf.fit(aed_X, aed_y)\n",
        "\n",
        "        aed_knn_scores = cross_val_score(aed_knn_grided_cv, aed_X, aed_y, cv=10)\n",
        "        print(aed_knn_scores)\n",
        "        plot_confusion_matrix(aed_knn_grided_cv, aed_X, aed_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(aed_dt_scores, aed_rf_scores, aed_knn_scores) \n",
        "\n",
        "print(\"p = \", p)\n",
        "print(f\"{'Rejeita' if p < 0.01 else 'Aceita'}-se a hipótese nula\")\n",
        "\n",
        "plt.boxplot([aed_dt_scores, aed_rf_scores, aed_knn_scores], labels=['Decision Tree', 'Random Forest', 'KNN'])\n",
        "# plt.ylim(0.8, 1)\n",
        "plt.title(\"AedesSex Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Dataset Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_scores = []\n",
        "model_scores.append([arr_dt_scores, arr_rf_scores, arr_knn_scores])\n",
        "model_scores.append([air_dt_scores, air_rf_scores, air_knn_scores])\n",
        "model_scores.append([pho_dt_scores, pho_rf_scores, pho_knn_scores])\n",
        "model_scores.append([phi_dt_scores, phi_rf_scores, phi_knn_scores])\n",
        "model_scores.append([sat_dt_scores, sat_rf_scores, sat_knn_scores])\n",
        "model_scores.append([adu_dt_scores, adu_rf_scores, adu_knn_scores])\n",
        "model_scores.append([aed_dt_scores, aed_rf_scores, aed_knn_scores])\n",
        "\n",
        "dataset_names = ['Arrythymia', 'Airlines', 'Phoneme', 'Phishing', 'Satellite', 'Adult', 'AedesSex']\n",
        "model_names = ['Decision Tree', 'Random Forest', 'KNN']\n",
        "labels = []\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "    for model_name in model_names:\n",
        "        labels.append(f\"{model_name} - {dataset_name}\")\n",
        "\n",
        "\n",
        "plt.boxplot(model_scores, labels=labels)\n",
        "plt.ylim(0.8, 1)\n",
        "plt.title(\"All Models Evaluation Scores\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Trabalho_1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "717656e8048df523d4225c89593711099144616bdcca6afd3cc05aa21074fef3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
